# Benchmark Folder Guide

This document explains what is stored in `benchmark/`, where to find outputs, and what `error_log` folders mean.

## Folder Layout

Top-level files:

- `results.csv`: benchmark results
- `verification_results.csv`: verification script results
- `results_accurary_issues.py`, `results_optimality_count.py`, `results_solve_time.py`:
  scripts used to analyze/plot benchmark outcomes.
- `benchmark_data.json`: metadata of the benchmark generation settings.

Subfolders:

- `graphs/`: input SDF3 graph instances (`.xml`) used by `solve_benchmark.py`.
- `opts/`: generator option templates (`.opt`) for benchmark sets.
- `results/`: detailed per-instance solver exports.
- `figures/`: per-instance schedule figures produced by model runs.
- `graph_figures/`: extra graph visualization assets.
- `skip/`: archived/excluded graph files used during data curation (not automatically consumed by `solve_benchmark.py`).
- `verification-results/`: reserved folder (currently empty in this repository state).

## Benchmark Instance Naming

Graph names follow this structure:

`graph__<actors>-<exec_avg>-<initial_tokens>-<procs_nr_types>_<index>.xml`

Example:

`graph__10-1000-0.10-02_0.xml`

From `benchmark/benchmark_data.json`, the identifier keys are:

- `actors`
- `exec_avg`
- `initial_tokens`
- `procs_nr_types`

## Where Run Outputs Go

When you run `python solve_benchmark.py`, each graph gets:

- `[benchmark_name]/results/<graph_id>/`
- `[benchmark_name]/figures/<graph_id>/`

Inside `[benchmark_name]/results/<graph_id>/`, you will typically find:

- Monolithic exports (one file set per method), e.g.:
  - `..._Quinton_Monolithic.txt/.lp/.json`
  - `..._Bounded_Monolithic.txt/.lp/.json`
  - `..._Bounded_Quinton_Monolithic.txt/.lp/.json`
- Benders exports, e.g.:
  - `..._Quinton_Benders_master.txt/.lp/.json`
  - `..._Quinton_Benders_sub.txt/.lp/.json`
  - `..._Quinton_Benders_aux.txt/.lp/.json`
  - `..._Quinton_Benders_primal_sub.txt/.lp/.json`
  - Similar sets for `Bounded_Benders` and `Bounded_Quinton_Benders`.

## Meaning of `error_log` Folders

Some instance result folders contain:

- `[benchmark_name]/results/<graph_id>/error_log/`

These are created by Benders runs when an exception occurs (for example CPLEX runtime errors).  
The code writes debug artifacts for master/sub/aux/primal-sub models with suffix `_ERRORLOG`, such as:

- `..._master_ERRORLOG.txt/.lp/.json`
- `..._sub_ERRORLOG.txt/.lp/.json`
- `..._aux_ERRORLOG.txt/.lp/.json`
- `..._primal_sub_ERRORLOG.txt/.lp/.json`

Use these files to inspect the exact model state that triggered the issue.

## Interpreting Summary CSVs

- `results.csv` (generated by `solve_benchmark.py`): primary aggregate benchmark table.
- `verification_results.csv` (generated by `python verify_benchmark.py`): per-technique verification verdicts (matching objectives, constraint violations, etc.).

Because multiple historical CSVs are kept, use the file expected by the plotting script you run.
